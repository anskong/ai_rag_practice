{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970fbe07",
   "metadata": {},
   "source": [
    "LLM - chatGPT, Anthropic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd496b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai key values ::: 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\n",
      "anthropic key values ::: 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\n",
      "huggingface_token::: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경변수 읽어오기\n",
    "load_dotenv(override=True)  # .env 파일을 덮어쓰기 모드로 읽기\n",
    "\n",
    "# 환경변수 불러오기\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "huggingface_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "print(f\"openai key values ::: {openai_key}\")  # 테스트용 (실제 서비스에서는 print 금지)\n",
    "print(f\"anthropic key values ::: {anthropic_key}\")  # 테스트용 (실제 서비스에서는 print 금지)\n",
    "print(f\"huggingface_token::: {huggingface_token}\")  # 테스트용 (실제 서비스에서는 print 금지)\n",
    "#...ddd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "716129a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anthropic key values ::: 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\n",
      "client values <class 'anthropic.Anthropic'> ::: <anthropic.Anthropic object at 0x00000188839F8640>\n",
      "[TextBlock(citations=None, text='Anthropic은 2021년에 설립된 미국 샌프란시스코에 기반을 둔 AI 연구 기업입니다. 이 회사는 딥러닝과 강화학습 기술을 기반으로 안전하고 윤리적인 인공지능 시스템 개발을 목표로 하고 있습니다. \\n\\nAnthropic의 주요 연구 분야는 다음과 같습니다:\\n\\n1. 보편 인공지능(Artificial General Intelligence, AGI) - 인간 수준의 지능을 갖춘 AI 개발 \\n\\n2. AI 안전성 - AI 시스템이 안전하고 제어 가능하도록 보장하는 방법 연구\\n\\n3. AI 윤리 - AI의 개발과 사용에 있어 윤리적 고려사항 연구\\n\\n4. AI의 투명성과 설명가능성 - AI 시스템의 의사결정 과정을 이해하고 설명할 수 있도록 하는 방법 연구\\n\\n대표적인 프로젝트로는 Constitutional AI가 있는데, 이는 AI 시스템에 헌법적 가치를 내재화하여 AI가 안전하고 윤리적으로 행동하도록 하는 것입니다. \\n\\nAnthropic은 현재 세계적 수준의 AI 연구진들로 구성되어 있으며, 오픈AI의 공동 설립자 중 한 명인 Dario Amodei가 이끌고 있습니다. 아직 신생 기업이지만 AI 윤리와 안전에 대한 Anthropic의 연구는 향후 AI 발전에 있어 중요한 역할을 할 것으로 기대됩니다.', type='text')]\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import httpx    \n",
    "\n",
    "\n",
    "\n",
    "print(f\"anthropic key values ::: {anthropic_key}\")\n",
    "\n",
    "client = anthropic.Anthropic(api_key=anthropic_key)\n",
    "\n",
    "httpx.Client(verify=False)\n",
    "\n",
    "client._client._transport._pool._ssl_context.check_hostname = False\n",
    "client._client._transport._pool._ssl_context.verify_mode = 0  # ssl.CERT_NONE\n",
    "\n",
    "print(\"client values {} ::: {}\".format(type(client), client) ) # 테스트용 (실제 서비스에서는 print 금지)\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Anthropic AI 소개해줘\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a413a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 저는 AI 언어 모델입니다. 사람들의 질문에 답하고 정보를 제공하여 도움이 되고자 만들어졌습니다. 다양한 주제에 걸쳐 대화를 나눌 수 있고, 글쓰기 지원, 번역, 정보 검색 등 여러 작업을 도울 수 있어요. 궁금한 것이 있으면 언제든지 물어보세요!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name = 'gpt-4o',\n",
    "    openai_api_key=openai_key\n",
    ")\n",
    "response = chat.invoke(\"너를 소개해줘\")\n",
    "\n",
    "print(response.content)  # 테스트용 (실제 서비스에서는 print 금지)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b62b1",
   "metadata": {},
   "source": [
    "# **Document Loader**\n",
    "## ***<span style=\"color:yellow\">PDF Loader</span>***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea700c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyPDF 설치\n",
    "# !pip install -q pypdf\n",
    "# G:\\내 드라이브\\문영호\\109 RFP MOH eng.pdf\n",
    "\n",
    "#PyPDFLoader 불러오기\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언\n",
    "loader = PyPDFLoader(r\"G:\\내 드라이브\\문영호\\109 RFP MOH eng.pdf\")\n",
    "\n",
    "# PDF파일 로드 및 페이지별로 자르기\n",
    "pages = loader.load_and_split()\n",
    "print(f\"페이지 수: {len(pages)}\")\n",
    "# print(pages[0].page_content)\n",
    "\n",
    "for i in range(5):   \n",
    "    print(f\"페이지 {i+1} 내용\") \n",
    "    print(\"===================================\")\n",
    "    print(pages[i].page_content)\n",
    "    print(pages[i].metadata)\n",
    "    print(pages[i].metadata['page'])\n",
    "    print(pages[i].metadata['page_label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422de17c",
   "metadata": {},
   "source": [
    "# **Text Splitter**\n",
    "\n",
    "<!-- **굵게 (bold)**  \n",
    "*기울임 (italic)*  \n",
    "***굵고 기울임*** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127676bf",
   "metadata": {},
   "source": [
    "# ***<sapn style=\"color:green\">Text Embbeding</span>***\n",
    "<!-- ## ***글자 크기 조정 2***\n",
    "###  **글자 크기 조정 3** -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "409985e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c8c9f",
   "metadata": {},
   "source": [
    "## **openAI Embedding Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3dec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#임베딩 모델 API 호출\n",
    "embeddings_model = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
    "\n",
    "#PDF 문서 로드\n",
    "# loader = PyPDFium2Loader(r\"G:\\내 드라이브\\문영호\\109 RFP MOH eng.pdf\")\n",
    "# loader = PyPDFium2Loader(r\"G:\\내 드라이브\\문영호\\알파고\\alphago.pdf\")\n",
    "loader = PyPDFium2Loader(r\"G:\\내 드라이브\\LLM-RAG-LangChain\\대한민국헌법(헌법)(제00010호)(19880225).pdf\")\n",
    "\n",
    "pages = loader.load()\n",
    "\n",
    "#PDF 문서를 여러 청크로 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "#청크 수 확인\n",
    "print(f\"청크 수: {len(texts)}\")\n",
    "#청크 내용 확인\n",
    "print(\"started printing chunk contents\")\n",
    "for i in range(10,15):   \n",
    "    print(f\"청크 {i+1} 내용\") \n",
    "    print(\"===================================\")\n",
    "    print(texts[i].page_content)\n",
    "    print(texts[i].metadata)\n",
    "    print(texts[i].metadata['page'])\n",
    "    # print(texts[i].metadata['page_label'])\n",
    "print(\"finished printing chunk contents\")\n",
    "\n",
    "#OpenAI 임베딩 모델로 청크들을 임베딩 변환하기\n",
    "embeddings = embeddings_model.embed_documents([i.page_content for i in texts])\n",
    "len(embeddings), len(embeddings[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9eb64",
   "metadata": {},
   "source": [
    "##### **[문장 유사도]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2dc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = [\n",
    "        \"안녕하세요\",\n",
    "        \"제 이름은 홍두깨입니다.\",\n",
    "        \"AI 초보자로서 잘 부탁드립니다.\",\n",
    "        \"열심히 배워서 회사의 발전에 이바지할 수 있는 인재로 성장하겠습니다.\",\n",
    "        \"LG CNS 만세 저는 아부쟁이입니다.\"\n",
    "    ]\n",
    "embeddings = embeddings_model.embed_documents(chunk)\n",
    "print(embeddings)\n",
    "\n",
    "# 임베딩 모델 API 호출 - embed_query\n",
    "embedded_query_q1 = embeddings_model.embed_query(\"이 대화에서 언급된 이름은 무엇입니까?\")\n",
    "embedded_query_q2 = embeddings_model.embed_query(\"이 대화에서 언급된 사람의 포부는 무엇입니까?\")\n",
    "embedded_query_q3 = embeddings_model.embed_query(\"이 대화에서 언급된 사람은 아부쟁이일까요 아닐까요?\")\n",
    "embedded_query_a = embeddings_model.embed_query(\"이 대화에서 언급된 이름은 홍길동입니다.\")\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "# 임베딩 간 유사도 함수\n",
    "def cos_sim(A, B):\n",
    "       return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    print(chunk[i])\n",
    "    print(cos_sim(embedded_query_q3, embeddings[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37847a",
   "metadata": {},
   "source": [
    "### **오픈소스 임베딩 모델 활용하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d29b9",
   "metadata": {},
   "source": [
    "**[jhgan/ko-sroberta-multitask 임베딩 모델 활용]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d8649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "# 임베딩 간 유사도 함수\n",
    "def cos_sim(A, B):\n",
    "       return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "a = \"dafadf\"\n",
    "\n",
    "#HuggingfaceEmbedding 함수로 Open source 임베딩 모델 로드\n",
    "# Hugging Face token : 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\n",
    "model_name = \"jhgan/ko-sroberta-multitask\"\n",
    "# model_name = \"yechan/KoreanSBERT-krissbert\"\n",
    "# model_name = \"intfloat/multilingual-e5-small\"\n",
    "ko_embedding= HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"token\": a}\n",
    ")\n",
    "\n",
    "examples = ko_embedding.embed_documents(\n",
    "     [\n",
    "        \"안녕하세요\",\n",
    "        \"제 이름은 홍두깨입니다.\",\n",
    "        \"이름이 무엇인가요?\",\n",
    "        \"랭체인은 유용합니다.\",\n",
    "     ]\n",
    " )\n",
    "\n",
    "embedded_query_q = ko_embedding.embed_query(\"이 대화에서 언급된 이름은 무엇입니까?\")\n",
    "embedded_query_a = ko_embedding.embed_query(\"이 대화에서 언급된 이름은 홍길동입니다.\")\n",
    "\n",
    "print(cos_sim(embedded_query_q, embedded_query_a))\n",
    "print(cos_sim(embedded_query_q, examples[1]))\n",
    "print(cos_sim(embedded_query_q, examples[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4c4d4",
   "metadata": {},
   "source": [
    "**[BAAI/bge-small-en 임베딩 모델 활용 코드]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67608f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "bge_embedding= HuggingFaceEmbeddings(\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "examples = bge_embedding.embed_documents(\n",
    "     [\n",
    "        \"안녕하세요\",\n",
    "        \"제 이름은 홍두깨입니다.\",\n",
    "        \"이름이 무엇인가요?\",\n",
    "        \"랭체인은 유용합니다.\",\n",
    "     ]\n",
    " )\n",
    "\n",
    "embedded_query_q = bge_embedding.embed_query(\"이 대화에서 언급된 이름은 무엇입니까?\")\n",
    "embedded_query_a = bge_embedding.embed_query(\"이 대화에서 언급된 이름은 홍길동입니다.\")\n",
    "\n",
    "print(cos_sim(embedded_query_q, embedded_query_a))\n",
    "print(cos_sim(embedded_query_q, examples[1]))\n",
    "print(cos_sim(embedded_query_q, examples[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
