{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970fbe07",
   "metadata": {},
   "source": [
    "LLM - chatGPT, Anthropic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd496b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경변수 읽어오기\n",
    "load_dotenv(override=True)  # .env 파일을 덮어쓰기 모드로 읽기\n",
    "\n",
    "# 환경변수 불러오기\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "huggingface_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "print(f\"openai key values ::: {openai_key}\")  # 테스트용 (실제 서비스에서는 print 금지)\n",
    "print(f\"anthropic key values ::: {anthropic_key}\")  # 테스트용 (실제 서비스에서는 print 금지)\n",
    "print(f\"huggingface_token::: {huggingface_token}\")  # 테스트용 (실제 서비스에서는 print 금지)\n",
    "#...ddd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from anthropic import Anthropic\n",
    "import httpx  \n",
    "\n",
    "vfy_client = httpx.Client(verify=False)\n",
    "\n",
    "# 1. 직접 Anthropic\n",
    "client = Anthropic(api_key=anthropic_key, http_client=vfy_client)\n",
    "\n",
    "# 2. Langchain Anthropic 모델 호출\n",
    "chat = ChatAnthropic(\n",
    "    model_name =\"claude-3-opus-20240229\",\n",
    "    anthropic_api_key=anthropic_key,)\n",
    "\n",
    "chat._client = client\n",
    "\n",
    "# chat._client._transport._pool._ssl_context.check_hostname = False\n",
    "# chat._client._transport._pool._ssl_context.verify_mode = 0  # ssl.CERT_NONE\n",
    "chat.invoke(\"안녕~ 너를 소개해줄래?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716129a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import httpx    \n",
    "\n",
    "\n",
    "\n",
    "print(f\"anthropic key values ::: {anthropic_key}\")\n",
    "\n",
    "vfy_client = httpx.Client(verify=False)\n",
    "\n",
    "client = anthropic.Anthropic(api_key=anthropic_key, http_client=vfy_client)\n",
    "\n",
    "\n",
    "# client._client._transport._pool._ssl_context.check_hostname = False\n",
    "# client._client._transport._pool._ssl_context.verify_mode = 0  # ssl.CERT_NONE\n",
    "\n",
    "# print(\"client values {} ::: {}\".format(type(client), client) ) # 테스트용 (실제 서비스에서는 print 금지)\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Anthropic AI 소개해줘\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a413a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name = 'gpt-4o',\n",
    "    openai_api_key=openai_key\n",
    ")\n",
    "response = chat.invoke(\"너를 소개해줘\")\n",
    "\n",
    "print(response.content)  # 테스트용 (실제 서비스에서는 print 금지)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b62b1",
   "metadata": {},
   "source": [
    "# **Document Loader**\n",
    "## ***<span style=\"color:yellow\">PDF Loader</span>***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea700c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyPDF 설치\n",
    "# !pip install -q pypdf\n",
    "# G:\\내 드라이브\\문영호\\109 RFP MOH eng.pdf\n",
    "\n",
    "#PyPDFLoader 불러오기\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - window11 \n",
    "# loader = PyPDFLoader(r\"G:\\내 드라이브\\문영호\\109 RFP MOH eng.pdf\")\n",
    "loader = PyPDFLoader(\n",
    "    r\"G:\\내 드라이브\\LLM-RAG-LangChain\\대한민국헌법(헌법)(제00010호)(19880225).pdf\"\n",
    ")\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - macOS\n",
    "# loader = PyPDFLoader(\n",
    "#     r\"/Users/youngho_moon/Library/CloudStorage/GoogleDrive-anskong@gmail.com/내 드라이브/LLM-RAG-LangChain/대한민국헌법(헌법)(제00010호)(19880225).pdf\"\n",
    "# )\n",
    "\n",
    "# PDF파일 로드 및 페이지별로 자르기\n",
    "pages = loader.load_and_split()\n",
    "print(f\"페이지 수: {len(pages)}\")\n",
    "# print(pages[0].page_content)\n",
    "\n",
    "for i in range(5):   \n",
    "    print(f\"페이지 {i+1} 내용\") \n",
    "    print(\"===================================\")\n",
    "    print(pages[i].page_content)\n",
    "    print(pages[i].metadata)\n",
    "    print(pages[i].metadata['page'])\n",
    "    print(pages[i].metadata['page_label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422de17c",
   "metadata": {},
   "source": [
    "# **Text Splitter**\n",
    "\n",
    "<!-- **굵게 (bold)**  \n",
    "*기울임 (italic)*  \n",
    "***굵고 기울임*** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127676bf",
   "metadata": {},
   "source": [
    "# ***<sapn style=\"color:green\">Text Embbeding</span>***\n",
    "<!-- ## ***글자 크기 조정 2***\n",
    "###  **글자 크기 조정 3** -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409985e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c8c9f",
   "metadata": {},
   "source": [
    "## **openAI Embedding Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3dec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#임베딩 모델 API 호출\n",
    "embeddings_model = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
    "\n",
    "#PDF 문서 로드\n",
    "# loader = PyPDFium2Loader(r\"G:\\내 드라이브\\문영호\\109 RFP MOH eng.pdf\")\n",
    "# loader = PyPDFium2Loader(r\"G:\\내 드라이브\\문영호\\알파고\\alphago.pdf\")\n",
    "# loader = PyPDFium2Loader(r\"G:\\내 드라이브\\LLM-RAG-LangChain\\대한민국헌법(헌법)(제00010호)(19880225).pdf\")\n",
    "loader = PyPDFium2Loader(\n",
    "    r\"/Users/youngho_moon/Library/CloudStorage/GoogleDrive-anskong@gmail.com/내 드라이브/LLM-RAG-LangChain/대한민국헌법(헌법)(제00010호)(19880225).pdf\"\n",
    ")\n",
    "# PDF파일 로드 및 페이지별로 자르기\n",
    "pages = loader.load()\n",
    "\n",
    "#PDF 문서를 여러 청크로 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "#청크 수 확인\n",
    "print(f\"청크 수: {len(texts)}\")\n",
    "#청크 내용 확인\n",
    "print(\"started printing chunk contents\")\n",
    "for i in range(1,2):   \n",
    "    print(f\"청크 {i+1} 내용\") \n",
    "    print(\"===================================\")\n",
    "    print(texts[i].page_content)\n",
    "    print(texts[i].metadata)\n",
    "    print(texts[i].metadata['page'])\n",
    "    # print(texts[i].metadata['page_label'])\n",
    "print(\"finished printing chunk contents\")\n",
    "\n",
    "#OpenAI 임베딩 모델로 청크들을 임베딩 변환하기\n",
    "embeddings = embeddings_model.embed_documents([i.page_content for i in texts])\n",
    "len(embeddings), len(embeddings[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9eb64",
   "metadata": {},
   "source": [
    "##### **[문장 유사도]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2dc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = [\n",
    "        \"안녕하세요\",\n",
    "        \"제 이름은 홍두깨입니다.\",\n",
    "        \"AI 초보자로서 잘 부탁드립니다.\",\n",
    "        \"열심히 배워서 회사의 발전에 이바지할 수 있는 인재로 성장하겠습니다.\",\n",
    "        \"LG CNS 만세 저는 아부쟁이입니다.\"\n",
    "    ]\n",
    "embeddings = embeddings_model.embed_documents(chunk)\n",
    "print(embeddings)\n",
    "\n",
    "# 임베딩 모델 API 호출 - embed_query\n",
    "embedded_query_q1 = embeddings_model.embed_query(\"이 대화에서 언급된 이름은 무엇입니까?\")\n",
    "embedded_query_q2 = embeddings_model.embed_query(\"이 대화에서 언급된 사람의 포부는 무엇입니까?\")\n",
    "embedded_query_q3 = embeddings_model.embed_query(\"이 대화에서 언급된 사람은 아부쟁이일까요 아닐까요?\")\n",
    "embedded_query_a = embeddings_model.embed_query(\"이 대화에서 언급된 이름은 홍길동입니다.\")\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "# 임베딩 간 유사도 함수\n",
    "def cos_sim(A, B):\n",
    "       return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    print(chunk[i])\n",
    "    print(cos_sim(embedded_query_q3, embeddings[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37847a",
   "metadata": {},
   "source": [
    "### **오픈소스 임베딩 모델 활용하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d29b9",
   "metadata": {},
   "source": [
    "**[jhgan/ko-sroberta-multitask 임베딩 모델 활용]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d8649e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "jhgan/ko-sbert-nli does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#HuggingfaceEmbedding 함수로 Open source 임베딩 모델 로드\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# model_name = \"jhgan/ko-sroberta-multitask\"\u001b[39;00m\n\u001b[32m     16\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mjhgan/ko-sbert-nli\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m ko_embedding= \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m examples = ko_embedding.embed_documents(\n\u001b[32m     22\u001b[39m      [\n\u001b[32m     23\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m안녕하세요\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m      ]\n\u001b[32m     28\u001b[39m  )\n\u001b[32m     30\u001b[39m embedded_query_q = ko_embedding.embed_query(\u001b[33m\"\u001b[39m\u001b[33m이 대화에서 언급된 이름은 무엇입니까?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:224\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    223\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:92\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43msentence_transformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:309\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[32m    303\u001b[39m     model_name_or_path,\n\u001b[32m    304\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    308\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    322\u001b[39m         model_name_or_path,\n\u001b[32m    323\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         config_kwargs=config_kwargs,\n\u001b[32m    331\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1808\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[32m   1806\u001b[39m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[32m   1807\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1810\u001b[39m     module = module_class.load(model_name_or_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:81\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     78\u001b[39m     config_args = {}\n\u001b[32m     80\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     84\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:181\u001b[39m, in \u001b[36mTransformer._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28mself\u001b[39m.auto_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_peft_model:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_peft_model(model_name_or_path, config, cache_dir, **model_args, **adapter_only_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_utils.py:4260\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4251\u001b[39m     gguf_file\n\u001b[32m   4252\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4253\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4254\u001b[39m ):\n\u001b[32m   4255\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4256\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4260\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4262\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4267\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4273\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4278\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4279\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_utils.py:1080\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[39m\n\u001b[32m   1072\u001b[39m has_file_kwargs = {\n\u001b[32m   1073\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrevision\u001b[39m\u001b[33m\"\u001b[39m: revision,\n\u001b[32m   1074\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproxies\u001b[39m\u001b[33m\"\u001b[39m: proxies,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1077\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlocal_files_only\u001b[39m\u001b[33m\"\u001b[39m: local_files_only,\n\u001b[32m   1078\u001b[39m }\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, **has_file_kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1080\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m   1081\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have a file named\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1082\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but there is a file for TensorFlow weights.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1083\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Use `from_tf=True` to load this model from those weights.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1084\u001b[39m     )\n\u001b[32m   1085\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, **has_file_kwargs):\n\u001b[32m   1086\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m   1087\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have a file named\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1088\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but there is a file for Flax weights. Use\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1089\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `from_flax=True` to load this model from those weights.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1090\u001b[39m     )\n",
      "\u001b[31mOSError\u001b[39m: jhgan/ko-sbert-nli does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "# pip install sentence-transformers\n",
    "# pip install --upgrade sentence-transformers  ->  필요\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "# 임베딩 간 유사도 함수\n",
    "def cos_sim(A, B):\n",
    "       return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "#HuggingfaceEmbedding 함수로 Open source 임베딩 모델 로드\n",
    "# model_name = \"jhgan/ko-sroberta-multitask\"\n",
    "# 한국어 NLI 기반 SBERT, RAG용으로 최적화된 모델\n",
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "ko_embedding= HuggingFaceEmbeddings(\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "examples = ko_embedding.embed_documents(\n",
    "     [\n",
    "        \"안녕하세요\",\n",
    "        \"제 이름은 홍두깨입니다.\",\n",
    "        \"이름이 무엇인가요?\",\n",
    "        \"랭체인은 유용합니다.\",\n",
    "     ]\n",
    " )\n",
    "\n",
    "embedded_query_q = ko_embedding.embed_query(\"이 대화에서 언급된 이름은 무엇입니까?\")\n",
    "embedded_query_a = ko_embedding.embed_query(\"이 대화에서 언급된 이름은 홍길동입니다.\")\n",
    "\n",
    "print(cos_sim(embedded_query_q, embedded_query_a))\n",
    "print(cos_sim(embedded_query_q, examples[1]))\n",
    "print(cos_sim(embedded_query_q, examples[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4c4d4",
   "metadata": {},
   "source": [
    "**[BAAI/bge-small-en 임베딩 모델 활용 코드]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67608f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "BAAI/bge-small-en does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m      3\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mBAAI/bge-small-en\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m bge_embedding= \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m examples = bge_embedding.embed_documents(\n\u001b[32m      9\u001b[39m      [\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m안녕하세요\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m      ]\n\u001b[32m     15\u001b[39m  )\n\u001b[32m     17\u001b[39m embedded_query_q = bge_embedding.embed_query(\u001b[33m\"\u001b[39m\u001b[33m이 대화에서 언급된 이름은 무엇입니까?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:224\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    223\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:92\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43msentence_transformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:309\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[32m    303\u001b[39m     model_name_or_path,\n\u001b[32m    304\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    308\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    322\u001b[39m         model_name_or_path,\n\u001b[32m    323\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         config_kwargs=config_kwargs,\n\u001b[32m    331\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1808\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[32m   1806\u001b[39m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[32m   1807\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1810\u001b[39m     module = module_class.load(model_name_or_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:81\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     78\u001b[39m     config_args = {}\n\u001b[32m     80\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     84\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:181\u001b[39m, in \u001b[36mTransformer._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28mself\u001b[39m.auto_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_peft_model:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_peft_model(model_name_or_path, config, cache_dir, **model_args, **adapter_only_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_utils.py:4260\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4251\u001b[39m     gguf_file\n\u001b[32m   4252\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4253\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4254\u001b[39m ):\n\u001b[32m   4255\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4256\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4260\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4262\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4267\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4273\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4278\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4279\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_utils.py:1100\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[39m\n\u001b[32m   1094\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m   1095\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have a file named\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1096\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but there is a file without the variant\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1097\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Use `variant=None` to load this model from those weights.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1098\u001b[39m                 )\n\u001b[32m   1099\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m   1101\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have a file named\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1104\u001b[39m                 )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted\u001b[39;00m\n\u001b[32m   1108\u001b[39m     \u001b[38;5;66;03m# to the original exception.\u001b[39;00m\n\u001b[32m   1109\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: BAAI/bge-small-en does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack."
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "bge_embedding= HuggingFaceEmbeddings(\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "examples = bge_embedding.embed_documents(\n",
    "     [\n",
    "        \"안녕하세요\",\n",
    "        \"제 이름은 홍두깨입니다.\",\n",
    "        \"이름이 무엇인가요?\",\n",
    "        \"랭체인은 유용합니다.\",\n",
    "     ]\n",
    " )\n",
    "\n",
    "embedded_query_q = bge_embedding.embed_query(\"이 대화에서 언급된 이름은 무엇입니까?\")\n",
    "embedded_query_a = bge_embedding.embed_query(\"이 대화에서 언급된 이름은 홍길동입니다.\")\n",
    "\n",
    "print(cos_sim(embedded_query_q, embedded_query_a))\n",
    "print(cos_sim(embedded_query_q, examples[1]))\n",
    "print(cos_sim(embedded_query_q, examples[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586ae4f",
   "metadata": {},
   "source": [
    "## **문서 벡터 저장소, Vector Stores**\n",
    "### **Langchain-Chroma 문서 저장 및 유사 문서 검색**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "openai_embedding=OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - window11 \n",
    "# loader = PyPDFLoader(r\"G:\\내 드라이브\\문영호\\109 RFP MOH eng.pdf\")\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - macOS\n",
    "loader = PyPDFLoader(\n",
    "    r\"/Users/youngho_moon/Library/CloudStorage/GoogleDrive-anskong@gmail.com/내 드라이브/LLM-RAG-LangChain/대한민국헌법(헌법)(제00010호)(19880225).pdf\"\n",
    ")\n",
    "\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "db = Chroma.from_documents(docs, openai_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c931aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"대통령의 임기는?\"\n",
    "#유사 문서 검색\n",
    "docs = db.similarity_search(query)\n",
    "for doc in docs:\n",
    "    print(\"===================================\")\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)\n",
    "    print(doc.metadata['page'])\n",
    "    # print(doc.metadata['page_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f601860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#유사 문서 검색 및 유사도 출력\n",
    "db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e85f78",
   "metadata": {},
   "source": [
    "**<span style='color:green'>[벡터DB를 로컬 디스크에 저장하고 로드하기]</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chroma().delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ac890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - macOS\n",
    "loader = PyPDFLoader(\n",
    "    r\"/Users/youngho_moon/Library/CloudStorage/GoogleDrive-anskong@gmail.com/내 드라이브/LLM-RAG-LangChain/대한민국헌법(헌법)(제00010호)(19880225).pdf\"\n",
    ")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "\n",
    "#HuggingfaceEmbedding 함수로 Open source 임베딩 모델 로드\n",
    "model_name = \"jhgan/ko-sroberta-multitask\"\n",
    "ko_embedding= HuggingFaceEmbeddings(\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "#save to local disk,\n",
    "db2 = Chroma.from_documents(docs, ko_embedding, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42381e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=ko_embedding)\n",
    "\n",
    "query = \"대통령의 임기는?\"\n",
    "result = db3.similarity_search(query)\n",
    "print(result[0].page_content)\n",
    "\n",
    "# # load from disk\n",
    "# db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=ko_embedding)\n",
    "\n",
    "# query = \"대통령의 임기는?\"\n",
    "# result = db3.similarity_search(query)\n",
    "# print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffeace1",
   "metadata": {},
   "source": [
    "## **RAG의 핵심, 문서 검색기 Retriever**\n",
    "### **Retriever의 기본형, 벡터DB 기반 Retriever**\n",
    "**<span style='color:yellow'>Chroma 벡터 DB 기반 기본 유사 문서 검색</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#헌법 PDF 파일 로드\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - macOS\n",
    "loader = PyPDFLoader(\n",
    "    r\"/Users/youngho_moon/Library/CloudStorage/GoogleDrive-anskong@gmail.com/내 드라이브/LLM-RAG-LangChain/대한민국헌법(헌법)(제00010호)(19880225).pdf\"\n",
    ")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "#PDF 파일을 500자 청크로 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "#ChromaDB에 청크들을 벡터 임베딩으로 저장(OpenAI 임베딩 모델 활용)\n",
    "db = Chroma.from_documents(docs, OpenAIEmbeddings(model = 'text-embedding-3-small'))\n",
    "\n",
    "#Chroma를 Retriever로 활용\n",
    "retriever = db.as_retriever()\n",
    "retriever.invoke(\"국회의원의 의무\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ecd68",
   "metadata": {},
   "source": [
    "**(1) 검색 결과 수 및 조정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e84dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#유사 청크 1개만 반환\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 10})\n",
    "retriever.invoke(\"국회의원의 의무\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2b754",
   "metadata": {},
   "source": [
    "**(2) 검색 방식 변경 - MMR**\n",
    "\n",
    "MMR은 쿼리에 대한 (1) 각 문서의 유사성 점수와 (2) 이미 선택된 문서들과의 다양성점수를 조합하여, 각 문서의 최종 점수를 계산합니다.\n",
    "\n",
    "$\\text{MMR} = \\lambda \\cdot \\text{Sim}(d, Q) - (1 - \\lambda) \\cdot \\max_{d' \\in D'} \\text{Sim}(d, d')$\n",
    "\n",
    "즉 유사 문서 후보군 중 특정 문서가 후보군 내 문서들 간의 유사성은 낮고, 쿼리와의 유사성이 높은 경우 점수가 높습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "#헌법 PDF 파일 로드\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - macOS\n",
    "loader = PyPDFLoader(\n",
    "    r\"/Users/youngho_moon/Library/CloudStorage/GoogleDrive-anskong@gmail.com/내 드라이브/LLM-RAG-LangChain/대한민국헌법(헌법)(제00010호)(19880225).pdf\"\n",
    ")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "#PDF 파일을 500자 청크로 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "#ChromaDB에 청크들을 벡터 임베딩으로 저장(OpenAI 임베딩 모델 활용)\n",
    "db = Chroma.from_documents(docs, OpenAIEmbeddings(model = 'text-embedding-3-small'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d011ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma를 Retriever로 활용\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs = {\"lambda_mult\": 0, \"fetch_k\":10, \"k\":3}\n",
    ")\n",
    "result = retriever.invoke(\"국회의원의 의무\")\n",
    "for idx, value in enumerate(result):\n",
    "  print(f\"{idx+1}번째 유사 문서:\")\n",
    "  print(value.page_content[:100])\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa09ae9",
   "metadata": {},
   "source": [
    "**<span style='color:yellow'>일반 유사도 검색 방식**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defda85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma를 Retriever로 활용\n",
    "retriever = db.as_retriever(search_kwargs = {\"k\":3})\n",
    "result = retriever.invoke(\"국회의원의 의무\")\n",
    "for idx, value in enumerate(result):\n",
    "  print(f\"{idx+1}번째 유사 문서:\")\n",
    "  print(value.page_content[:100])\n",
    "  print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
