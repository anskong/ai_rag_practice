{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a0cab0",
   "metadata": {},
   "source": [
    "## **RAG의 핵심, 문서 검색기 Retriever**\n",
    "### **사용자의 쿼리를 재해석하여 검색하다, MultiQueryRetriever**\n",
    "**Chroma DB에 문서 벡터 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경변수 읽어오기\n",
    "load_dotenv(override=True)  # .env 파일을 덮어쓰기 모드로 읽기\n",
    "\n",
    "# 환경변수 불러오기\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "huggingface_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "print(f\"openai key values ::: {openai_key}\")  # 테스트용 (실제 서비스에서는 print 금지)\n",
    "print(f\"anthropic key values ::: {anthropic_key}\")  # 테스트용 (실제 서비스에서는 print 금지)\n",
    "print(f\"huggingface_token::: {huggingface_token}\")  # 테스트용 (실제 서비스에서는 print 금지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "result = embedding_model.embed_query(\"테스트 문장입니다.\")\n",
    "print(result[:5])  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e738e4b",
   "metadata": {},
   "source": [
    "Test 소스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599490ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "#헌법 PDF 파일 로드\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - window11 \n",
    "loader = PyPDFLoader(r\"G:\\내 드라이브\\LLM-RAG-LangChain\\대한민국헌법(헌법제1호).pdf\")\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - macOS\n",
    "# loader = PyPDFLoader(\n",
    "#     r\"/Users/youngho_moon/Library/CloudStorage/GoogleDrive-anskong@gmail.com/내 드라이브/LLM-RAG-LangChain/대한민국헌법(헌법)(제00010호)(19880225).pdf\"\n",
    "# )\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "#PDF 파일을 500자 청크로 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "# docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# Chroma를 제거하고 임베딩만 추출\n",
    "docs = text_splitter.split_documents(pages[:3])  # 소량만 사용\n",
    "\n",
    "# 임베딩 결과 확인\n",
    "embedding_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "for doc in docs:\n",
    "    vec = embedding_model.embed_query(doc.page_content)\n",
    "    print(vec[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings                           \n",
    "from langchain_community.vectorstores import Chroma\n",
    "# from langchain_chroma import Chroma\n",
    "\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "# 기본 로깅 설정\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# ChromaDB 관련 로거 활성화\n",
    "logging.getLogger(\"chromadb\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"chromadb.db\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"chromadb.telemetry\").setLevel(logging.INFO)\n",
    "\n",
    "#헌법 PDF 파일 로드\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - window11 \n",
    "loader = PyPDFLoader(r\"G:\\내 드라이브\\LLM-RAG-LangChain\\대한민국헌법(헌법제1호).pdf\")\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - macOS\n",
    "# loader = PyPDFLoader(\n",
    "#     r\"/Users/youngho_moon/Library/CloudStorage/GoogleDrive-anskong@gmail.com/내 드라이브/LLM-RAG-LangChain/대한민국헌법(헌법)(제00010호)(19880225).pdf\"\n",
    "# )\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "#PDF 파일을 500자 청크로 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages[:5])\n",
    "\n",
    "#ChromaDB에 청크들을 벡터 임베딩으로 저장(OpenAI 임베딩 모델 활용)\n",
    "# db = Chroma.from_documents(docs, OpenAIEmbeddings(model = 'text-embedding-3-small'))\n",
    "# db = Chroma.from_documents(\n",
    "#     docs,\n",
    "#     OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "#     persist_directory=\"./chroma_db\"  # 폴더 직접 지정\n",
    "# )\n",
    "\n",
    "shutil.rmtree(\"C:/temp/chroma_db\", ignore_errors=True)\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# db = Chroma.from_documents(docs, embedding_model, persist_directory=\"C:/temp/chroma_db\")\n",
    "\n",
    "\n",
    "db = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"C:/temp/chroma_db\",\n",
    "    collection_name=\"my_collection\"\n",
    ")\n",
    "\n",
    "# 소규모 배치로 나누어 삽입\n",
    "batch_size = 5\n",
    "for i in range(0, len(docs), batch_size):\n",
    "    batch = docs[i:i + batch_size]\n",
    "    db.add_documents(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b9c67",
   "metadata": {},
   "source": [
    "**질문을 여러 버전으로 재해석하여 Retriever에 활용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#```Chroma DB에 대한민국 헌법 PDF 임베딩 변환 및 저장하는 과정은 위 셀에 있습니다```\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "#질문 문장 question으로 저장\n",
    "question = \"국회의원의 의무는 무엇이 있나요?\"\n",
    "#여러 버전의 질문으로 변환하는 역할을 맡을 LLM 선언\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\",\n",
    "                 temperature = 0)\n",
    "#MultiQueryRetriever에 벡터DB 기반 Retriever와 LLM 선언\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=db.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "# 여러 버전의 문장 생성 결과를 확인하기 위한 로깅 과정\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "#여러 버전 질문 생성 결과와 유사 청크 검색 개수 출력\n",
    "unique_docs = retriever_from_llm.invoke(input=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aa2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a2c88",
   "metadata": {},
   "source": [
    "### **<span style=\"color:yellow\">재정렬 기법(Reorder or Rerank)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b6068",
   "metadata": {},
   "source": [
    "**[Long-Context Reorder 없이 유사 문서 출력]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bafe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma dimension 관련 에러 발생 시 실행\n",
    "# Chroma().delete_collection()\n",
    "\n",
    "# pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e40e7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:56\u001b[39m, in \u001b[36mdependable_faiss_import\u001b[39m\u001b[34m(no_avx2)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'faiss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     10\u001b[39m texts = [\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m바스켓볼은 훌륭한 스포츠입니다.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m플라이 미 투 더 문은 제가 가장 좋아하는 노래 중 하나입니다.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m래리 버드는 상징적인 NBA 선수였습니다.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m ]\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Chroma Retriever 선언(10개의 유사 문서 출력)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m retriever = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext-embedding-3-small\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.as_retriever(\n\u001b[32m     24\u001b[39m     search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m10\u001b[39m}\n\u001b[32m     25\u001b[39m )\n\u001b[32m     26\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33m셀틱에 대해 설명해줘\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 유사도 기준으로 검색 결과 출력\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1044\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1043\u001b[39m embeddings = embedding.embed_documents(texts)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:996\u001b[39m, in \u001b[36mFAISS.__from\u001b[39m\u001b[34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[39m\n\u001b[32m    984\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__from\u001b[39m(\n\u001b[32m    986\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    994\u001b[39m     **kwargs: Any,\n\u001b[32m    995\u001b[39m ) -> FAISS:\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     faiss = \u001b[43mdependable_faiss_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m distance_strategy == DistanceStrategy.MAX_INNER_PRODUCT:\n\u001b[32m    998\u001b[39m         index = faiss.IndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[32m0\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MOON YOUNGHO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:58\u001b[39m, in \u001b[36mdependable_faiss_import\u001b[39m\u001b[34m(no_avx2)\u001b[39m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import faiss python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     60\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     61\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     62\u001b[39m     )\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m faiss\n",
      "\u001b[31mImportError\u001b[39m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "texts = [\n",
    "    \"바스켓볼은 훌륭한 스포츠입니다.\",\n",
    "    \"플라이 미 투 더 문은 제가 가장 좋아하는 노래 중 하나입니다.\",\n",
    "    \"셀틱스는 제가 가장 좋아하는 팀입니다.\",\n",
    "    \"이것은 보스턴 셀틱스에 관한 문서입니다.\"\n",
    "    \"저는 단순히 영화 보러 가는 것을 좋아합니다\",\n",
    "    \"보스턴 셀틱스가 20점차로 이겼어요\",\n",
    "    \"이것은 그냥 임의의 텍스트입니다.\",\n",
    "    \"엘든 링은 지난 15 년 동안 최고의 게임 중 하나입니다.\",\n",
    "    \"L. 코넷은 최고의 셀틱스 선수 중 한 명입니다.\",\n",
    "    \"래리 버드는 상징적인 NBA 선수였습니다.\",\n",
    "]\n",
    "# Chroma Retriever 선언(10개의 유사 문서 출력)\n",
    "retriever = FAISS.from_texts(texts, OpenAIEmbeddings(model = 'text-embedding-3-small')).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "query = \"셀틱에 대해 설명해줘\"\n",
    "\n",
    "# 유사도 기준으로 검색 결과 출력\n",
    "docs = retriever.invoke(query)\n",
    "for i in docs:\n",
    "  print(i.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94605217",
   "metadata": {},
   "source": [
    "**[Long-Context Reorder 활용하여 유사 문서 출력]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3de8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LongContextReorder 선언\n",
    "reordering = LongContextReorder()\n",
    "#검색된 유사문서 중 관련도가 높은 문서를 맨앞과 맨뒤에 재정배치\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "for i in reordered_docs:\n",
    "  print(i.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef4ae4",
   "metadata": {},
   "source": [
    "**필요없는 문서는 삭제, Contextual Compression, 맥락 압축 기법(Context Compression)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c23c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "loader = PyPDFLoader(r\"/content/drive/MyDrive/마소캠퍼스/대한민국 헌법.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "db = FAISS.from_documents(docs, OpenAIEmbeddings(model = 'text-embedding-3-small'))\n",
    "retriever =db.as_retriever()\n",
    "\n",
    "docs = retriever.invoke(\"대통령의 임기는?\") \n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0238bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = ChatOpenAI(model ='gpt-4o-mini', temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"대통령의 임기는?\"\n",
    ")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e0bc8",
   "metadata": {},
   "source": [
    "### **<span style=\"color:yellow\">가상 문서로 유사 문서 탐색, HyDE(Hypothetical Document Embedding)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"\n",
    "당신은 LangChain, LangGraph, LangServe, LangSmith라는 LLM 기반 애플리케이션을 구축하기 위한 일련의 소프트웨어에 대한 전문가입니다.\n",
    "\n",
    "LangChain은 LLM 애플리케이션을 구축하기 위해 쉽게 구성할 수 있는 대규모 통합 세트를 제공하는 Python 프레임워크입니다.\n",
    "LangGraph는 상태 저장, 멀티 액터 LLM 애플리케이션을 쉽게 구축할 수 있는 LangChain 위에 구축된 Python 패키지입니다.\n",
    "LangServe는 REST API로 LangChain 애플리케이션을 쉽게 배포할 수 있는 LangChain 위에 구축된 Python 패키지입니다.\n",
    "LangSmith는 LLM 애플리케이션 추적 및 테스트를 쉽게 할 수 있는 플랫폼입니다.\n",
    "\n",
    "사용자 질문에 최선을 다해 답변하세요. 사용자 질문에 대한 튜토리얼을 작성하는 것처럼 답변하세요.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "qa_no_context = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = qa_no_context.invoke(\n",
    "    {\n",
    "        \"question\": \"Chain을 구축할 때 멀티모달 모델을 활용하는 방법과 Chain을 REST API로 전환하는 방법\"\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "hyde_chain = RunnablePassthrough.assign(hypothetical_document=qa_no_context)\n",
    "\n",
    "hyde_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"Chain을 구축할 때 멀티모달 모델을 활용하는 방법과 Chain을 REST API로 전환하는 방법\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
