{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a0cab0",
   "metadata": {},
   "source": [
    "## **RAG의 핵심, 문서 검색기 Retriever**\n",
    "### **사용자의 쿼리를 재해석하여 검색하다, MultiQueryRetriever**\n",
    "**Chroma DB에 문서 벡터 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경변수 읽어오기\n",
    "load_dotenv(override=True)  # .env 파일을 덮어쓰기 모드로 읽기\n",
    "\n",
    "# 환경변수 불러오기\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "huggingface_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "print(f\"openai key values ::: {openai_key}\")  # 테스트용 (실제 서비스에서는 print 금지)\n",
    "print(f\"anthropic key values ::: {anthropic_key}\")  # 테스트용 (실제 서비스에서는 print 금지)\n",
    "print(f\"huggingface_token::: {huggingface_token}\")  # 테스트용 (실제 서비스에서는 print 금지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "result = embedding_model.embed_query(\"테스트 문장입니다.\")\n",
    "print(result[:5])  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e738e4b",
   "metadata": {},
   "source": [
    "Test 소스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599490ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "#헌법 PDF 파일 로드\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - window11 \n",
    "loader = PyPDFLoader(r\"G:\\내 드라이브\\LLM-RAG-LangChain\\대한민국헌법(헌법제1호).pdf\")\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - macOS\n",
    "# loader = PyPDFLoader(\n",
    "#     r\"/Users/youngho_moon/Library/CloudStorage/GoogleDrive-anskong@gmail.com/내 드라이브/LLM-RAG-LangChain/대한민국헌법(헌법)(제00010호)(19880225).pdf\"\n",
    "# )\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "#PDF 파일을 500자 청크로 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "# docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# Chroma를 제거하고 임베딩만 추출\n",
    "docs = text_splitter.split_documents(pages[:3])  # 소량만 사용\n",
    "\n",
    "# 임베딩 결과 확인\n",
    "embedding_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "for doc in docs:\n",
    "    vec = embedding_model.embed_query(doc.page_content)\n",
    "    print(vec[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings                           \n",
    "from langchain_community.vectorstores import Chroma\n",
    "# from langchain_chroma import Chroma\n",
    "\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "# 기본 로깅 설정\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# ChromaDB 관련 로거 활성화\n",
    "# logging.getLogger(\"chromadb\").setLevel(logging.DEBUG)\n",
    "# logging.getLogger(\"chromadb.db\").setLevel(logging.DEBUG)\n",
    "# logging.getLogger(\"chromadb.telemetry\").setLevel(logging.INFO)\n",
    "\n",
    "#헌법 PDF 파일 로드\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - window11 \n",
    "# loader = PyPDFLoader(r\"G:\\내 드라이브\\LLM-RAG-LangChain\\대한민국헌법(헌법제1호).pdf\")\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - macOS\n",
    "loader = PyPDFLoader(\n",
    "    r\"/Users/youngho_moon/Library/CloudStorage/GoogleDrive-anskong@gmail.com/내 드라이브/LLM-RAG-LangChain/대한민국헌법(헌법)(제00010호)(19880225).pdf\"\n",
    ")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "#PDF 파일을 500자 청크로 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages[:5])\n",
    "\n",
    "#ChromaDB에 청크들을 벡터 임베딩으로 저장(OpenAI 임베딩 모델 활용)\n",
    "# db = Chroma.from_documents(docs, OpenAIEmbeddings(model = 'text-embedding-3-small'))\n",
    "# db = Chroma.from_documents(\n",
    "#     docs,\n",
    "#     OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "#     persist_directory=\"./chroma_db\"  # 폴더 직접 지정\n",
    "# )\n",
    "\n",
    "# shutil.rmtree(\"C:/temp/chroma_db\", ignore_errors=True)\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# shutil.rmtree(\"./chroma_db\", ignore_errors=True)\n",
    "db = Chroma.from_documents(docs, embedding_model, persist_directory=\"./chroma_db\")\n",
    "\n",
    "# db = Chroma(\n",
    "#     embedding_function=embedding_model,\n",
    "#     persist_directory=\"C:/temp/chroma_db\",\n",
    "#     collection_name=\"my_collection\"\n",
    "# )\n",
    "\n",
    "# # 소규모 배치로 나누어 삽입\n",
    "# batch_size = 5\n",
    "# for i in range(0, len(docs), batch_size):\n",
    "#     batch = docs[i:i + batch_size]\n",
    "#     db.add_documents(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b9c67",
   "metadata": {},
   "source": [
    "**질문을 여러 버전으로 재해석하여 Retriever에 활용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#```Chroma DB에 대한민국 헌법 PDF 임베딩 변환 및 저장하는 과정은 위 셀에 있습니다```\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "#질문 문장 question으로 저장\n",
    "question = \"국회의원의 의무는 무엇이 있나요?\"\n",
    "#여러 버전의 질문으로 변환하는 역할을 맡을 LLM 선언\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\",\n",
    "                 temperature = 0)\n",
    "#MultiQueryRetriever에 벡터DB 기반 Retriever와 LLM 선언\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=db.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "# 여러 버전의 문장 생성 결과를 확인하기 위한 로깅 과정\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "#여러 버전 질문 생성 결과와 유사 청크 검색 개수 출력\n",
    "unique_docs = retriever_from_llm.invoke(input=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aa2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a2c88",
   "metadata": {},
   "source": [
    "### **<span style=\"color:yellow\">재정렬 기법(Reorder or Rerank)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b6068",
   "metadata": {},
   "source": [
    "**[Long-Context Reorder 없이 유사 문서 출력]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bafe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma dimension 관련 에러 발생 시 실행\n",
    "# Chroma().delete_collection()\n",
    "\n",
    "# pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e40e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "texts = [\n",
    "    \"바스켓볼은 훌륭한 스포츠입니다.\",\n",
    "    \"플라이 미 투 더 문은 제가 가장 좋아하는 노래 중 하나입니다.\",\n",
    "    \"셀틱스는 제가 가장 좋아하는 팀입니다.\",\n",
    "    \"이것은 보스턴 셀틱스에 관한 문서입니다.\"\n",
    "    \"저는 단순히 영화 보러 가는 것을 좋아합니다\",\n",
    "    \"보스턴 셀틱스가 20점차로 이겼어요\",\n",
    "    \"이것은 그냥 임의의 텍스트입니다.\",\n",
    "    \"엘든 링은 지난 15 년 동안 최고의 게임 중 하나입니다.\",\n",
    "    \"L. 코넷은 최고의 셀틱스 선수 중 한 명입니다.\",\n",
    "    \"래리 버드는 상징적인 NBA 선수였습니다.\",\n",
    "]\n",
    "# Chroma Retriever 선언(10개의 유사 문서 출력)\n",
    "retriever = FAISS.from_texts(texts, OpenAIEmbeddings(model = 'text-embedding-3-small')).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "query = \"셀틱에 대해 설명해줘\"\n",
    "\n",
    "# 유사도 기준으로 검색 결과 출력\n",
    "docs = retriever.invoke(query)\n",
    "for i in docs:\n",
    "  print(i.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94605217",
   "metadata": {},
   "source": [
    "**[Long-Context Reorder 활용하여 유사 문서 출력]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3de8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "#LongContextReorder 선언\n",
    "reordering = LongContextReorder()\n",
    "#검색된 유사문서 중 관련도가 높은 문서를 맨앞과 맨뒤에 재정배치\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "for i in reordered_docs:\n",
    "  print(i.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef4ae4",
   "metadata": {},
   "source": [
    "**필요없는 문서는 삭제, Contextual Compression, 맥락 압축 기법(Context Compression)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c23c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Helper function for printing docs\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# PDF파일 불러올 객체 PyPDFLoader 선언 - macOS\n",
    "loader = PyPDFLoader(\n",
    "    r\"/Users/youngho_moon/Library/CloudStorage/GoogleDrive-anskong@gmail.com/내 드라이브/LLM-RAG-LangChain/대한민국헌법(헌법)(제00010호)(19880225).pdf\"\n",
    ")\n",
    "pages = loader.load_and_split()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "db = FAISS.from_documents(docs, OpenAIEmbeddings(model = 'text-embedding-3-small'))\n",
    "retriever =db.as_retriever()\n",
    "\n",
    "docs = retriever.invoke(\"대통령의 임기는?\") \n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0238bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model ='gpt-4o-mini', temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"대통령의 임기는?\"\n",
    ")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e0bc8",
   "metadata": {},
   "source": [
    "### **<span style=\"color:yellow\">가상 문서로 유사 문서 탐색, HyDE(Hypothetical Document Embedding)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fc1851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"\n",
    "당신은 LangChain, LangGraph, LangServe, LangSmith라는 LLM 기반 애플리케이션을 구축하기 위한 일련의 소프트웨어에 대한 전문가입니다.\n",
    "\n",
    "LangChain은 LLM 애플리케이션을 구축하기 위해 쉽게 구성할 수 있는 대규모 통합 세트를 제공하는 Python 프레임워크입니다.\n",
    "LangGraph는 상태 저장, 멀티 액터 LLM 애플리케이션을 쉽게 구축할 수 있는 LangChain 위에 구축된 Python 패키지입니다.\n",
    "LangServe는 REST API로 LangChain 애플리케이션을 쉽게 배포할 수 있는 LangChain 위에 구축된 Python 패키지입니다.\n",
    "LangSmith는 LLM 애플리케이션 추적 및 테스트를 쉽게 할 수 있는 플랫폼입니다.\n",
    "\n",
    "사용자 질문에 최선을 다해 답변하세요. 사용자 질문에 대한 튜토리얼을 작성하는 것처럼 답변하세요.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "qa_no_context = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e46d189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain을 구축할 때 멀티모달 모델을 활용하고 이를 REST API로 전환하는 방법에 대해 단계별로 설명하겠습니다. 이 튜토리얼에서는 LangChain과 LangServe를 사용하여 멀티모달 Chain을 구축하고 이를 REST API로 배포하는 과정을 다룹니다.\n",
      "\n",
      "### 1단계: 멀티모달 모델 준비하기\n",
      "\n",
      "멀티모달 모델은 텍스트, 이미지, 오디오 등 다양한 형태의 데이터를 처리할 수 있는 모델입니다. 예를 들어, OpenAI의 CLIP 모델은 텍스트와 이미지를 함께 처리할 수 있습니다. 먼저 필요한 라이브러리를 설치합니다.\n",
      "\n",
      "```bash\n",
      "pip install langchain openai\n",
      "```\n",
      "\n",
      "### 2단계: 멀티모달 Chain 구축하기\n",
      "\n",
      "이제 LangChain을 사용하여 멀티모달 Chain을 구축해 보겠습니다. 아래는 텍스트와 이미지를 입력으로 받아 처리하는 간단한 Chain의 예입니다.\n",
      "\n",
      "```python\n",
      "from langchain import LLMChain\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "# OpenAI API 키 설정\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
      "\n",
      "# 텍스트와 이미지를 처리하는 프롬프트 템플릿 생성\n",
      "prompt_template = PromptTemplate(\n",
      "    input_variables=[\"text\", \"image\"],\n",
      "    template=\"Given the text: {text} and the image: {image}, provide a detailed analysis.\"\n",
      ")\n",
      "\n",
      "# LLM 초기화\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "\n",
      "# Chain 생성\n",
      "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "\n",
      "# Chain 테스트\n",
      "response = chain.run(text=\"A beautiful sunset\", image=\"path/to/sunset_image.jpg\")\n",
      "print(response)\n",
      "```\n",
      "\n",
      "### 3단계: Chain을 REST API로 전환하기\n",
      "\n",
      "이제 LangServe를 사용하여 위에서 만든 Chain을 REST API로 배포해 보겠습니다. LangServe를 설치합니다.\n",
      "\n",
      "```bash\n",
      "pip install langserve\n",
      "```\n",
      "\n",
      "이제 REST API 서버를 설정합니다.\n",
      "\n",
      "```python\n",
      "from langserve import LangServe\n",
      "\n",
      "# LangServe 애플리케이션 생성\n",
      "app = LangServe()\n",
      "\n",
      "# Chain을 API 엔드포인트로 등록\n",
      "@app.route(\"/analyze\", methods=[\"POST\"])\n",
      "def analyze():\n",
      "    data = request.json\n",
      "    text = data.get(\"text\")\n",
      "    image = data.get(\"image\")\n",
      "    response = chain.run(text=text, image=image)\n",
      "    return {\"response\": response}\n",
      "\n",
      "# 서버 실행\n",
      "if __name__ == \"__main__\":\n",
      "    app.run(port=5000)\n",
      "```\n",
      "\n",
      "### 4단계: API 테스트하기\n",
      "\n",
      "이제 REST API가 준비되었습니다. Postman이나 curl을 사용하여 API를 테스트할 수 있습니다. 아래는 curl을 사용한 예시입니다.\n",
      "\n",
      "```bash\n",
      "curl -X POST http://localhost:5000/analyze \\\n",
      "-H \"Content-Type: application/json\" \\\n",
      "-d '{\"text\": \"A beautiful sunset\", \"image\": \"path/to/sunset_image.jpg\"}'\n",
      "```\n",
      "\n",
      "### 결론\n",
      "\n",
      "이 튜토리얼에서는 LangChain을 사용하여 멀티모달 Chain을 구축하고, LangServe를 통해 이를 REST API로 전환하는 방법을 살펴보았습니다. 이제 여러분은 텍스트와 이미지를 함께 처리하는 멀티모달 애플리케이션을 REST API 형태로 배포할 수 있습니다. 추가적인 기능이나 개선 사항은 필요에 따라 구현할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "answer = qa_no_context.invoke(\n",
    "    {\n",
    "        \"question\": \"Chain을 구축할 때 멀티모달 모델을 활용하는 방법과 Chain을 REST API로 전환하는 방법\"\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56b5897f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Chain을 구축할 때 멀티모달 모델을 활용하는 방법과 Chain을 REST API로 전환하는 방법',\n",
       " 'hypothetical_document': 'Chain을 구축할 때 멀티모달 모델을 활용하고, 이를 REST API로 전환하는 방법에 대해 단계별로 설명하겠습니다. 이 튜토리얼에서는 LangChain과 LangServe를 사용하여 멀티모달 Chain을 구축하고 이를 REST API로 배포하는 과정을 다룹니다.\\n\\n### 1단계: 멀티모달 모델 준비하기\\n\\n멀티모달 모델은 텍스트, 이미지, 오디오 등 다양한 형태의 데이터를 처리할 수 있는 모델입니다. 예를 들어, OpenAI의 CLIP 모델은 텍스트와 이미지를 함께 처리할 수 있습니다. 먼저 필요한 라이브러리를 설치합니다.\\n\\n```bash\\npip install langchain\\npip install torch torchvision\\n```\\n\\n### 2단계: 멀티모달 Chain 구축하기\\n\\n이제 LangChain을 사용하여 멀티모달 Chain을 구축해 보겠습니다. 아래는 텍스트와 이미지를 입력으로 받아 처리하는 간단한 Chain의 예입니다.\\n\\n```python\\nfrom langchain import Chain\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import SimpleSequentialChain\\n\\n# 텍스트와 이미지를 처리할 LLM 초기화\\nllm = OpenAI(model=\"gpt-3.5-turbo\")\\n\\n# 텍스트 프롬프트 템플릿 생성\\ntext_prompt = PromptTemplate(\\n    input_variables=[\"text\"],\\n    template=\"이 텍스트에 대한 설명을 제공하세요: {text}\"\\n)\\n\\n# 이미지 프롬프트 템플릿 생성\\nimage_prompt = PromptTemplate(\\n    input_variables=[\"image\"],\\n    template=\"이 이미지를 설명하세요: {image}\"\\n)\\n\\n# Chain 구성\\ntext_chain = Chain(llm=llm, prompt=text_prompt)\\nimage_chain = Chain(llm=llm, prompt=image_prompt)\\n\\n# 멀티모달 Chain 생성\\nmulti_modal_chain = SimpleSequentialChain(chains=[text_chain, image_chain])\\n```\\n\\n### 3단계: Chain 테스트하기\\n\\nChain이 잘 작동하는지 확인하기 위해 테스트 입력을 제공해 보겠습니다.\\n\\n```python\\ntext_input = \"이것은 아름다운 풍경입니다.\"\\nimage_input = \"path/to/image.jpg\"  # 이미지 경로\\n\\n# Chain 실행\\ntext_output = multi_modal_chain.run({\"text\": text_input, \"image\": image_input})\\nprint(text_output)\\n```\\n\\n### 4단계: Chain을 REST API로 전환하기\\n\\n이제 LangServe를 사용하여 위에서 만든 Chain을 REST API로 배포해 보겠습니다. LangServe를 설치합니다.\\n\\n```bash\\npip install langserve\\n```\\n\\n이제 REST API 서버를 설정합니다.\\n\\n```python\\nfrom langserve import LangServe\\n\\n# LangServe 초기화\\napp = LangServe()\\n\\n# Chain을 API 엔드포인트로 등록\\n@app.route(\"/multi_modal\", methods=[\"POST\"])\\ndef multi_modal_api():\\n    data = request.json\\n    text_input = data.get(\"text\")\\n    image_input = data.get(\"image\")\\n    \\n    # Chain 실행\\n    output = multi_modal_chain.run({\"text\": text_input, \"image\": image_input})\\n    return jsonify({\"output\": output})\\n\\n# 서버 실행\\nif __name__ == \"__main__\":\\n    app.run(port=5000)\\n```\\n\\n### 5단계: API 테스트하기\\n\\n이제 API가 준비되었습니다. Postman이나 curl을 사용하여 API를 테스트할 수 있습니다.\\n\\n```bash\\ncurl -X POST http://localhost:5000/multi_modal \\\\\\n-H \"Content-Type: application/json\" \\\\\\n-d \\'{\"text\": \"이것은 아름다운 풍경입니다.\", \"image\": \"path/to/image.jpg\"}\\'\\n```\\n\\n### 결론\\n\\n이 튜토리얼에서는 LangChain을 사용하여 멀티모달 Chain을 구축하고, LangServe를 통해 이를 REST API로 배포하는 방법을 살펴보았습니다. 이 과정을 통해 다양한 입력 형식을 처리하는 LLM 애플리케이션을 쉽게 만들 수 있습니다. 추가적인 질문이 있다면 언제든지 문의해 주세요!'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "hyde_chain = RunnablePassthrough.assign(hypothetical_document=qa_no_context)\n",
    "\n",
    "hyde_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"Chain을 구축할 때 멀티모달 모델을 활용하는 방법과 Chain을 REST API로 전환하는 방법\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yhmoon05",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
